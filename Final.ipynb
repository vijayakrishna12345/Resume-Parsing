{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import io\n",
    "import glob\n",
    "import random\n",
    "from pathlib import Path\n",
    "import spacy\n",
    "import docx2txt\n",
    "import PyPDF2\n",
    "import csv\n",
    "import pandas\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "from nltk.corpus import wordnet\n",
    "from itertools import islice\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "os.chdir(\"C:/Users/Krish/Desktop/Review\")\n",
    "docDir = \"C:/Users/Krish/Desktop/Review/Resumes/\"\n",
    "txtDir = \"C:/Users/Krish/Desktop/Review/Text/\"\n",
    "newDir = \"C:/Users/Krish/Desktop/Review/New/\"\n",
    "import mysql.connector\n",
    "file_name_dict={}\n",
    "file_mail_dict={}\n",
    "file_ph_dict={}\n",
    "file_exp_dict={}\n",
    "file_skill_dict={}\n",
    "result_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of stop words: 305\n"
     ]
    }
   ],
   "source": [
    "#function to remove the words from the spacy's stop words vocabulary\n",
    "print('Number of stop words: %d' % len(STOP_WORDS))\n",
    "\n",
    "removeStop = [ 'around', 'as', 'at', 'been', 'between', 'during', 'eight', 'eleven', 'fifteen', 'fifty', 'five' , 'for',\n",
    "              'forty', 'four', 'from', 'hundred', 'nine', 'since', 'six', 'sixty', 'ten', 'three', 'twelve', 'twenty', 'two', \n",
    "             'upon', 'of', 'to']\n",
    "for stop in removeStop:\n",
    "    STOP_WORDS.remove(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "777\n"
     ]
    }
   ],
   "source": [
    "#function to add the words to the spacy's stop words vocabulary\n",
    "with open('Stop.txt','r') as f:\n",
    "    customize_stop_words =  f.read().split(',')\n",
    "#print(customize_stop_words)    \n",
    "for word in customize_stop_words:\n",
    "    STOP_WORDS.add(word)\n",
    "for word in STOP_WORDS:    \n",
    "    lexeme=nlp.vocab[word]\n",
    "    lexeme.is_stop=True\n",
    "print(len(STOP_WORDS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function which the file extensions and if it is docx or pdf it calls the functions to convert them into txt format\n",
    "def convertMultiple(docDir, txtDir):\n",
    "    file_list=[]\n",
    "    for doc in os.listdir(docDir): \n",
    "        doc_len=len(doc.split(\".\"))\n",
    "        fileExtension = doc.split(\".\")[-1]\n",
    "        dname=doc.split(\".\")[0]\n",
    "        print(dname)\n",
    "        if fileExtension == \"docx\":\n",
    "            #function to convert a docx file to a txt file\n",
    "            docFilename = docDir + str(doc) \n",
    "            text = docx2txt.process(docFilename) \n",
    "            textFilename = txtDir + dname + \".txt\"\n",
    "            textFile = open(textFilename, \"w\", encoding=\"UTF-8\") \n",
    "            textFile.write(text)\n",
    "            textFile.close()\n",
    "            \n",
    "        elif fileExtension == \"pdf\":\n",
    "            docFilename = docDir + str(doc) \n",
    "            textFilename = txtDir + dname + \".txt\"\n",
    "            text = pdftotext(docFilename,textFilename) \n",
    "            \n",
    "            \n",
    "        elif fileExtension ==\"txt\":\n",
    "            docFilename = docDir + str(doc) \n",
    "            textFilename = txtDir + dname + \".txt\"\n",
    "            textFile = open(textFilename, \"w\", encoding=\"UTF-8\") \n",
    "            textFile.write(text)\n",
    "            textFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to convert a pdf file to a txt file \n",
    "def pdftotext(file,textFilename):\n",
    "    content = \"\"\n",
    "    p1 = open(file, \"rb\")\n",
    "    pdf = PyPDF2.PdfFileReader(p1)\n",
    "    num=pdf.getNumPages()\n",
    "    for i in range(0, num):\n",
    "        content += pdf.getPage(i).extractText() + \"\\n\"\n",
    "    content = \" \".join(content.replace(u\"\\xa0\", u\" \").strip().split())\n",
    "    new=\"\"\n",
    "    for i in content:\n",
    "        new+=i\n",
    "    textFile = open(textFilename, \"w\", encoding=\"UTF-8\") \n",
    "    textFile.write(new)\n",
    "    textFile.close()\n",
    "    p1.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to find e-mail ids\n",
    "def email(newDir):\n",
    "    path = newDir + \"*.txt\"\n",
    "    path = path.replace(os.sep, '/')\n",
    "    text_files = glob.glob(path)\n",
    "    files = set(text_files)\n",
    "    files = list(files)\n",
    "    for each in files:\n",
    "        filename=each.split('\\\\')[-1]\n",
    "        filename=filename.split('.')[0]\n",
    "        #print(filename)                    \n",
    "        with open(each, 'r', encoding='utf8', errors='ignore') as f:\n",
    "            search = f.read()\n",
    "        email = None           \n",
    "        pattern = re.compile('[\\w\\.-]+@[\\w\\.-]+')\n",
    "        matches = pattern.findall(search)\n",
    "        email = matches\n",
    "        email = set(email)\n",
    "        email = list(email)\n",
    "        new = \"\" \n",
    "  \n",
    "      \n",
    "        for x in email: \n",
    "            new += x+\" , \" \n",
    "        print(new)    \n",
    "        file_mail_dict[filename]=new\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to find phone numbers\n",
    "def phone(file):\n",
    "    path = newDir + \"*.txt\"\n",
    "    path = path.replace(os.sep, '/')\n",
    "    text_files = glob.glob(path)\n",
    "    files = set(text_files)\n",
    "    files = list(files)   \n",
    "    for each in files:\n",
    "        filename=each.split('\\\\')[-1]\n",
    "        filename=filename.split('.')[0]\n",
    "        with open(each, 'r', encoding='utf8', errors='ignore') as f:\n",
    "            search = f.read()\n",
    "        phone = None           \n",
    "        pattern = re.compile(r'([+(]?\\d+[)\\-]?[ \\t\\r\\f\\v]*[(]?\\d{2,}[()\\-]?[ \\t\\r\\f\\v]*\\d{2,}[()\\-]?[ \\t\\r\\f\\v]*\\d*[ \\t\\r\\f\\v]*\\d*[ \\t\\r\\f\\v]*)')\n",
    "        matches = pattern.findall(search)\n",
    "        matches = [re.sub(r'[,.]', '', el) for el in matches if len(re.sub(r'[()\\-.,\\s+]', '', el))>6]\n",
    "        matches = [re.sub(r'\\D$', '', el).strip() for el in matches]\n",
    "        matches = [el for el in matches if len(re.sub(r'\\D','',el)) <= 15]\n",
    "        for el in list(matches):\n",
    "            if len(el.split('-')) > 3: continue\n",
    "            for x in el.split(\"-\"):\n",
    "                try:\n",
    "                    if x.strip()[-4:].isdigit():\n",
    "                        if int(x.strip()[-4:]) in range(1900, 2100):\n",
    "                            matches.remove(el)\n",
    "                except: pass                  \n",
    "        phone = matches\n",
    "        phone = set(phone)\n",
    "        phone = list(phone)\n",
    "        new = \"\" \n",
    "  \n",
    "      \n",
    "        for x in phone: \n",
    "            new += x+\" , \" \n",
    "        print(new) \n",
    "        file_ph_dict[filename]=new\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to find the skillsets\n",
    "def skillsets():\n",
    "    \n",
    "    from gensim.models import Word2Vec\n",
    "    model = Word2Vec.load('java_model.bin')\n",
    "    vector=model.wv\n",
    "    path = newDir + \"*.txt\"\n",
    "    path = path.replace(os.sep, '/')\n",
    "    #print(path)\n",
    "    text_files = glob.glob(path)\n",
    "    files = set(text_files)\n",
    "    files = list(files) \n",
    "    #print(files)\n",
    "    for each in files:\n",
    "        filename=each.split('\\\\')[-1]\n",
    "        filename=filename.split('.')[0]\n",
    "        extension=each.split('.')[-1]\n",
    "        if extension == 'txt':\n",
    "            \n",
    "        \n",
    "            skillsets=set() \n",
    "            with open(each, 'r', encoding='utf8', errors='ignore') as f:\n",
    "                search = f.read().lower().split()   \n",
    "                    \n",
    "                for i in search:\n",
    "                    #compares each word in the file with java\n",
    "                    if i in vector.vocab and i.isdigit()==False:                    \n",
    "                        res=vector.similarity('java',i)\n",
    "    \n",
    "                    #stores only those values which are greater than 94%\n",
    "                        if res>0.94 :\n",
    "                            skillsets.add(i)\n",
    "        new=\"\"        \n",
    "        \n",
    "        skillsets=list(skillsets)\n",
    "        for x in skillsets: \n",
    "            new += x+\" , \" \n",
    "        new=tokenize1(new)    \n",
    "        print(new)\n",
    "        file_skill_dict[filename]=new\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to remove phone numbers for finding the name and experience\n",
    "def remove_phone(file):\n",
    "    with open(file, 'r', encoding='utf8', errors='ignore') as f:\n",
    "        search = f.read()\n",
    "    try:\n",
    "    \n",
    "        phone = None           \n",
    "        pattern = re.compile(r'([+(]?\\d+[)\\-]?[ \\t\\r\\f\\v]*[(]?\\d{2,}[()\\-]?[ \\t\\r\\f\\v]*\\d{2,}[()\\-]?[ \\t\\r\\f\\v]*\\d*[ \\t\\r\\f\\v]*\\d*[ \\t\\r\\f\\v]*)')\n",
    "        matches = pattern.findall(search)\n",
    "        matches = [re.sub(r'[,.]', '', el) for el in matches if len(re.sub(r'[()\\-.,\\s+]', '', el))>6]\n",
    "        matches = [re.sub(r'\\D$', '', el).strip() for el in matches]\n",
    "        matches = [el for el in matches if len(re.sub(r'\\D','',el)) <= 15]\n",
    "        for el in list(matches):\n",
    "            if len(el.split('-')) > 3: continue\n",
    "            for x in el.split(\"-\"):\n",
    "                try:\n",
    "                    if x.strip()[-4:].isdigit():\n",
    "                        if int(x.strip()[-4:]) in range(1900, 2100):\n",
    "                            matches.remove(el)\n",
    "                except: pass                  \n",
    "        phone = matches\n",
    "        phone = set(phone)\n",
    "        phone = list(phone)\n",
    "        search = \"\"\n",
    "        with open(file, 'r', encoding='utf8', errors='ignore') as f:\n",
    "                search = f.read()\n",
    "                for i in phone:\n",
    "                    search = search.replace(i,\" \")\n",
    "        with open(file, \"w\",encoding='utf8', errors='ignore') as f:\n",
    "            f.write(search)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to remove e-mails  for finding the name and experience\n",
    "def remove_email(file):\n",
    "    with open(file, 'r', encoding='utf8', errors='ignore') as f:\n",
    "        search = f.read()\n",
    "    try:    \n",
    "        email = None           \n",
    "        pattern = re.compile('[\\w\\.-]+@[\\w\\.-]+')\n",
    "        matches = pattern.findall(search)\n",
    "        email = matches\n",
    "        email = set(email)\n",
    "        email = list(email)\n",
    "        search = \"\"\n",
    "        with open(file, 'r', encoding='utf8', errors='ignore') as f:\n",
    "                search = f.read()\n",
    "                for i in email:\n",
    "                    search = search.replace(i,\" \")\n",
    "        with open(file, \"w\",encoding='utf8', errors='ignore') as f:\n",
    "            f.write(search)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to find the name\n",
    "def name(file):\n",
    "    filename=file.split('\\\\')[-1]\n",
    "    filename=filename.split('.')[0]\n",
    "    with open(file, 'r', encoding='utf8', errors='ignore') as f:\n",
    "        search = f.read().upper()\n",
    "           \n",
    "    Sentences = nltk.sent_tokenize(search)\n",
    "    Tokens = []\n",
    "    for Sent in Sentences:\n",
    "        Tokens.append(nltk.word_tokenize(Sent)) \n",
    "    Words_List = [nltk.pos_tag(Token) for Token in Tokens]\n",
    "\n",
    "    Nouns_List = []\n",
    "\n",
    "    for List in Words_List:\n",
    "        for Word in List:\n",
    "            if re.match('[NN.*]', Word[1]):\n",
    "                Nouns_List.append(Word[0])\n",
    "\n",
    "    Names = []\n",
    "    for Nouns in Nouns_List:\n",
    "        if not wordnet.synsets(Nouns):\n",
    "                Names.append(Nouns)\n",
    "    \n",
    "    \n",
    "    Name = ' '.join(Names[:1])\n",
    "    print(\"Name : \", Name)\n",
    "    file_name_dict[filename]=Name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to find the experience\n",
    "def exp(file):\n",
    "\n",
    "    filename=file.split('\\\\')[-1]\n",
    "    filename=filename.split('.')[0]\n",
    "    with open(file, 'r', encoding='utf8', errors='ignore') as f:\n",
    "        search = f.read()\n",
    "    try: \n",
    "    #print(search)\n",
    "        new_left1 = []\n",
    "        new_left2 = []\n",
    "        left = 0\n",
    "        \n",
    "        left2 = re.search(r\"(?:[a-zA-Z'-]+[^a-zA-Z'-]+){0,7}experience(?:[^a-zA-Z'-]+[a-zA-Z'-]+){0,7}\", search)\n",
    "        if(left2 != None):\n",
    "            left = left2.group()\n",
    "        left4 = re.search(r\"(?:[a-zA-Z'-]+[^a-zA-Z'-]+){0,2}years(?:[^a-zA-Z'-]+[a-zA-Z'-]+){0,2}\", search)\n",
    "        if(left4 != None):\n",
    "            left = left4.group()\n",
    "        left5 = re.search(r\"(?:[a-zA-Z'-]+[^a-zA-Z'-]+){0,2}year(?:[^a-zA-Z'-]+[a-zA-Z'-]+){0,2}\", search)\n",
    "        if(left5 != None):\n",
    "            left = left5.group()    \n",
    "    \n",
    "        if(left == 0):\n",
    "            print(\"Experience : \", 0)\n",
    "            return\n",
    "    #print(left)\n",
    "        left1 = re.findall('[0-9]{1,2}',left)\n",
    "        left1_int = list(map(int, left1))\n",
    "        #print(left1_int)\n",
    "        for a in left1:\n",
    "            for b in left1_int:\n",
    "                if len(a) <= 2 and b < 30:\n",
    "                    new_left1.append(a)\n",
    "        left1 = ''.join(new_left1[0])\n",
    "        left2 = re.findall('[0-9]{1,2}.[0-9]{1,2}',left)\n",
    "    \n",
    "        exp = []\n",
    "        if not left2:\n",
    "            exp.append(left1)\n",
    "            exp = ''.join(left1)\n",
    "        else:\n",
    "            exp.append(left2)\n",
    "            exp = ''.join(left2)\n",
    "    except:\n",
    "        exp=0\n",
    "    #print(left1)\n",
    "    #print(left2)\n",
    "    #print(exp)\n",
    "    print(\"Experience : \", exp)\n",
    "    file_exp_dict[filename]=exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to call the functions which finds experience and name\n",
    "def name_and_exp_call():\n",
    "    path = newDir + \"*.txt\"\n",
    "    path = path.replace(os.sep, '/')\n",
    "    text_files = glob.glob(path)\n",
    "    files = set(text_files)\n",
    "    files = list(files)\n",
    "    #print(files)\n",
    "    for file in files:\n",
    "        #file = \"samiulla-h.txt\"\n",
    "        remove_phone(file)\n",
    "        remove_email(file)\n",
    "        name(file)\n",
    "        exp(file)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to store the extracted info in a csv\n",
    "def store_csv():\n",
    "    with open(\"resume.csv\",'w',encoding='utf8', errors='ignore') as fd:\n",
    "        writer=csv.writer(fd)\n",
    "        writer.writerow([\"ID\",\"Name\",\"Experience\",\"Skillset\",\"Phone no\",\"EmailID\"])\n",
    "        for key in file_name_dict.keys():\n",
    "            writer.writerow([key,file_name_dict[key],file_exp_dict.get(key,0),file_skill_dict[key],file_ph_dict[key],file_mail_dict[key]])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to calculate score and store the final result in a csv\n",
    "def score():\n",
    "    from gensim.models import Word2Vec\n",
    "    import csv\n",
    "    import math\n",
    "#loads the word2vec model created\n",
    "    model = Word2Vec.load('java_model.bin')\n",
    "    vector=model.wv\n",
    "\n",
    "    count_scr=0\n",
    "    score=0\n",
    "    maximum=0\n",
    "\n",
    "    result_dict = {}\n",
    "    #open the resume csv which has the job seeker's info like name,experience,etc\n",
    "    with open(\"resume.csv\",'r') as resume_csv_data:\n",
    "        csv_data = csv.DictReader(resume_csv_data)\n",
    "        for resume_row in csv_data:\n",
    "            ID = resume_row[\"ID\"]\n",
    "            experience = resume_row[\"Experience\"]\n",
    "            skills= tokenize1(resume_row[\"Skillset\"]).split(\",\")\n",
    "            #open the resume csv which has the companies info like job description,experience required,etc\n",
    "            job_csv = csv.DictReader(open('job_description11.csv', mode='r'))\n",
    "            for i,job_row in enumerate(job_csv):\n",
    "                job_experience_required=job_row[\"experience\"]\n",
    "                job_description=job_row[\"description\"]\n",
    "                job_ID = job_row[\"company\"]\n",
    "                job_description=tokenize1(job_description).split(',')\n",
    "                for skill in skills:                    \n",
    "                    for des in job_description:\n",
    "                        \n",
    "                            \n",
    "                        if skill in vector.vocab and des in vector.vocab:    \n",
    "                            similarity=vector.similarity(skill,des)\n",
    "                            if similarity > maximum:\n",
    "                                    maximum=similarity\n",
    "                            if similarity >=0.9999999999:    \n",
    "                                    count_scr=count_scr+1\n",
    "                    score=score+maximum\n",
    "#score is calculated by adding the maximum similarity skill with the experience of the job seeker and if any skill is exactly \n",
    "#matched it is multiplied by 0.2 and added to the final score\n",
    "                score=score+float(experience)+(count_scr*0.2)    \n",
    "                print(\"ID:{} Job{} Score: {}\".format(ID,job_ID,score))\n",
    "                if result_dict.get(job_ID,0)==0:\n",
    "                    result_dict[job_ID]=[[ID,score]]\n",
    "                else:\n",
    "                    result_dict[job_ID].append([ID,score])\n",
    "                score=0\n",
    "                count_scr=0\n",
    "                maximum=0\n",
    "    print(result_dict)\n",
    "\n",
    "    for each in result_dict.keys():\n",
    "        result_dict[each].sort(key=lambda x: x[1],reverse=True)\n",
    "        result_dict[each]=result_dict[each][:10]\n",
    "\n",
    "#storing the score info in a new csv - output_data    \n",
    "    import pandas as pd\n",
    "    #First update\n",
    "\n",
    "    \n",
    "    df = pd.read_csv(\"output_data.csv\")\n",
    "    if df.empty==True:\n",
    "        data=result_dict\n",
    "\n",
    "        with open(\"output_data.csv\",'w') as fp:\n",
    "            writer=csv.writer(fp)\n",
    "            writer.writerow([\"Company\",\"EmployeeID/Score\"])\n",
    "            for each in data.keys():\n",
    "                data[each].sort(key=lambda x : x[1],reverse=True)\n",
    "                writer.writerow([each,data[each][:3]])\n",
    "\n",
    "    #Dynamic update\n",
    "\n",
    "    #data=result_dict\n",
    "    else:\n",
    "        data=result_dict\n",
    "        n_dict={}\n",
    "\n",
    "        with open(\"output_data.csv\",'r') as fp:\n",
    "            reader=csv.reader(fp)\n",
    "            for i,row in enumerate(reader):\n",
    "                if i%2==0 and i>0:\n",
    "                    dat=list(row)\n",
    "                    lst=eval(dat[1])+data[dat[0]]\n",
    "                    n_dict[dat[0]]=lst\n",
    "\n",
    "        data=n_dict\n",
    "\n",
    "        with open(\"output_data.csv\",'w') as fp:\n",
    "            writer=csv.writer(fp)\n",
    "            writer.writerow([\"Company\",\"EmployeeID/Score\"])\n",
    "            for each in data.keys():\n",
    "                data[each].sort(key=lambda x : x[1],reverse=True)\n",
    "                writer.writerow([each,data[each][:3]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to pre-process the data\n",
    "def tokenize(string, newFilename):\n",
    "\n",
    "    doc = nlp(string)\n",
    "    str1 = '\\n'\n",
    "    for word in (doc.sents): #used for sentence tokenization\n",
    "        sentence = nlp(word.text) #used for text tokenization\n",
    "\n",
    "        for token in sentence:\n",
    "            #checks whether the word is a stop word , pronoun or a punctuation\n",
    "            if token.is_stop!=True and token.is_punct != True and token.pos_ != 'PRON':  \n",
    "                \n",
    "                str1 = str1 + str(token.lemma_) #stores the lemmatized word in the variable str1\n",
    "                with open(newFilename, \"a+\", encoding = \"UTF-8\") as f:\n",
    "\n",
    "                    f.write(str1)\n",
    "                    str1 = ' '\n",
    "        str1 = '\\n'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to refine the skillsets\n",
    "def tokenize1(string):\n",
    "    doc = nlp(string)\n",
    "    str1 = ' '\n",
    "    for word in (doc.sents):\n",
    "        sentence = nlp(word.text)\n",
    "\n",
    "        for token in sentence:\n",
    "            if token.is_stop!=True and token.is_punct != True and token.pos_ != 'PRON':\n",
    "                #print(token)\n",
    "                \n",
    "                \n",
    "                str1 = str1 + str(token.lemma_)+\",\"\n",
    "              \n",
    "    return str1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to check whether the file is in txt format\n",
    "def parse(txtDir):\n",
    "    path = txtDir + \"*.txt\"\n",
    "    path = path.replace(os.sep, '/')\n",
    "    text_files = glob.glob(path)\n",
    "    files = set(text_files)\n",
    "    files = list(files)   \n",
    "    for each in files:\n",
    "        each = each.replace(os.sep, '/')\n",
    "        each = os.path.basename(each)\n",
    "        \n",
    "        \n",
    "        tname, extension = each.split(\".\")\n",
    "        newFilename = newDir + str(tname) + \".txt\"\n",
    "        if extension == \"txt\":\n",
    "            textFilename = txtDir + str(each) \n",
    "        with open(textFilename, 'r', encoding=\"UTF-8\") as f:\n",
    "            string = f.read().lower()\n",
    "            tokenize(string, newFilename)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to remove the resumes parsed for info\n",
    "def remove_resumes():\n",
    "    import shutil\n",
    "    d='C:/Users/Krish/Desktop/Review/Resumes/' \n",
    "    filesToRemove = [os.path.join(d,f) for f in os.listdir(d)]\n",
    "    for f in filesToRemove:\n",
    "        os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to remove the files - the converted files and the preprocessed files\n",
    "def remove_text():\n",
    "    d='C:/Users/Krish/Desktop/Review/Text/' \n",
    "    filesToRemove = [os.path.join(d,f) for f in os.listdir(d)]\n",
    "    for f in filesToRemove:\n",
    "        os.remove(f)    \n",
    "    d='C:/Users/Krish/Desktop/Review/New/' \n",
    "    filesToRemove = [os.path.join(d,f) for f in os.listdir(d)]\n",
    "    for f in filesToRemove:\n",
    "        os.remove(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
